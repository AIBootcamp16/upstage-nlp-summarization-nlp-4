
# ===============================================================
# v4_no_lora_train.py â€” KoBART Full Fine-Tune (No LoRA)
#  - special tokens ì™„ì „ í•™ìŠµ
#  - v4 ì „ì²˜ë¦¬ì™€ 100% í˜¸í™˜
#  - decoder_start_token_id ëª…ì‹œ (KoBART)
#  - seed ì™„ì „ ê³ ì •
#  - train/eval decoding ì„¸íŒ… â‰’ inference ì„¸íŒ…ìœ¼ë¡œ ì •ë ¬
# ===============================================================

import os
import gc
import yaml
import torch
import optuna
import random
import numpy as np
import pandas as pd
import wandb

from datasets import Dataset
from transformers import (
    BartForConditionalGeneration,
    PreTrainedTokenizerFast,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    set_seed as hf_set_seed,
)
from rouge_score import rouge_scorer

torch.backends.cuda.matmul.allow_tf32 = True

# ===============================================================
# 0. Load Config
# ===============================================================
def load_config(path: str = "v4_config.yaml"):
    with open(path, "r") as f:
        return yaml.safe_load(f)

cfg = load_config()

DATA_DIR   = cfg["general"]["data_dir"]
TRAIN_FILE = os.path.join(DATA_DIR, cfg["general"]["train_file"])
DEV_FILE   = os.path.join(DATA_DIR, cfg["general"]["dev_file"])
MODEL_NAME = cfg["general"]["model_name"]
OUTPUT_DIR = cfg["general"]["output_dir"]
SEED_LIST  = cfg["general"]["seed_list"]
PREFIX     = cfg["general"]["prefix"]

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ===============================================================
# 1. Fix Seed
# ===============================================================
def fix_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    hf_set_seed(seed)

# ===============================================================
# 2. Dataset Load
# ===============================================================
def clean_text(x):
    if isinstance(x, list):
        return " ".join(map(str, x))
    return str(x)

def normalize_dialogue_column(df: pd.DataFrame) -> pd.DataFrame:
    def _fix(x):
        # Case 1: Python list ê·¸ëŒ€ë¡œ â†’ ê³µë°± join
        if isinstance(x, list):
            return " ".join(map(str, x))

        # Case 2: ë¬¸ìì—´ì¸ë° ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ìƒê¸´ ê²½ìš° â†’ eval í›„ join
        if isinstance(x, str) and x.strip().startswith("[") and x.strip().endswith("]"):
            try:
                arr = eval(x)
                if isinstance(arr, list):
                    return " ".join(map(str, arr))
            except:
                pass  # ì‹¤íŒ¨í•˜ë©´ ê·¸ëƒ¥ ë¬¸ìì—´ë¡œ ì²˜ë¦¬

        # Case 3: ì¼ë°˜ ë¬¸ìì—´ â†’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        return str(x)

    df["dialogue"] = df["dialogue"].apply(_fix)
    return df

# ===============================================================
# 2. Dataset Load (FINAL, list â†’ str ì™„ì „ ê°•ì œ ë³€í™˜ í¬í•¨)
# ===============================================================

def load_datasets():
    """
    v4 ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ì•ˆì „ì„±ì„ ìœ„í•´,
    - dialogue/summaryë¥¼ ë¬´ì¡°ê±´ ë¬¸ìì—´(str)ë¡œ ê°•ì œ ë³€í™˜
    - ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ìƒê¸´ ë¬¸ìì—´ë„ ì ˆëŒ€ literal_eval í•˜ì§€ ì•ŠìŒ
    - flatten / join ì²˜ë¦¬ë„ í•˜ì§€ ì•ŠìŒ (preprocess_builderì—ì„œ ì²˜ë¦¬)
    """

    train_df = pd.read_csv(TRAIN_FILE)
    dev_df   = pd.read_csv(DEV_FILE)

    # --- ëª¨ë“  ì…ë ¥ì„ ë¬´ì¡°ê±´ ë¬¸ìì—´(str)ë¡œ ë³€í™˜ ---
    train_df["dialogue"] = train_df["dialogue"].astype(str)
    dev_df["dialogue"]   = dev_df["dialogue"].astype(str)

    train_df["summary"] = train_df["summary"].astype(str)
    dev_df["summary"]   = dev_df["summary"].astype(str)

    # --- HuggingFace Dataset ë³€í™˜ ---
    train_ds = Dataset.from_pandas(train_df)
    dev_ds   = Dataset.from_pandas(dev_df)

    # --- map ë‹¨ê³„ì—ì„œëŠ” ë” ì´ìƒ ë³€í™˜ ì—†ìŒ ---
    return train_ds, dev_ds

# ===============================================================
# 3. Tokenizer
# ===============================================================
def build_tokenizer():
    tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)

    special_tokens = cfg["tokenizer"].get("special_tokens", [])
    if special_tokens:
        added = tokenizer.add_tokens(special_tokens)
        print(f"[INFO] Added {added} special tokens:", special_tokens)

    ENC_MAX = cfg["tokenizer"]["encoder_max_len"]
    DEC_MAX = cfg["tokenizer"]["decoder_max_len"]
    return tokenizer, ENC_MAX, DEC_MAX

# ===============================================================
# 4. Preprocess
# ===============================================================
def preprocess_builder(tokenizer, ENC_MAX, DEC_MAX):
    def preprocess(batch):
        import ast

        # 1) dialogue í‰íƒ„í™”
        dialogues = []
        for x in batch["dialogue"]:
            if isinstance(x, list):
                dialogues.append(" ".join(map(str, x)))
            elif isinstance(x, str) and x.startswith("[") and x.endswith("]"):
                try:
                    arr = ast.literal_eval(x)
                    if isinstance(arr, list):
                        dialogues.append(" ".join(map(str, arr)))
                        continue
                except:
                    pass
                dialogues.append(x)
            else:
                dialogues.append(str(x))

        # 2) summary í‰íƒ„í™”
        summaries = []
        for x in batch["summary"]:
            if isinstance(x, list):
                summaries.append(" ".join(map(str, x)))
            else:
                summaries.append(str(x))

        # 3) Tokenize
        inputs = [PREFIX + d for d in dialogues]

        enc = tokenizer(
            inputs,
            truncation=True,
            max_length=ENC_MAX,
            padding="max_length",
        )
        dec = tokenizer(
            summaries,
            truncation=True,
            max_length=DEC_MAX,
            padding="max_length",
        )

        # 4) Labels (-100 masking)
        labels = dec["input_ids"]
        pad_id = tokenizer.pad_token_id
        labels = [
            [-100 if t == pad_id else t for t in seq]
            for seq in labels
        ]
        enc["labels"] = labels

        # 5) KoBARTëŠ” token_type_ids ë¯¸ì§€ì› â†’ ì œê±°
        if "token_type_ids" in enc:
            del enc["token_type_ids"]
        if "token_type_ids" in dec:
            del dec["token_type_ids"]

        return enc

    return preprocess

# ===============================================================
# 5. ROUGE Metric
# ===============================================================
from rouge_score import rouge_scorer

def build_rouge_fn(tokenizer):
    scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)

    def compute_rouge(eval_preds):
        preds, labels = eval_preds
        pad_id = tokenizer.pad_token_id

        # -100 â†’ pad_id ë¡œ ë³µêµ¬ í›„ ë””ì½”ë”©
        labels = np.where(labels != -100, labels, pad_id)

        preds  = tokenizer.batch_decode(preds, skip_special_tokens=True)
        labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

        scores = [
            scorer.score(p, l)["rougeL"].fmeasure
            for p, l in zip(preds, labels)
        ]
        return {"rougeL": float(np.mean(scores))}
    
    return compute_rouge

# ===============================================================
# 6. Build Model â€” FULL FINETUNE (No LoRA)
# ===============================================================
# ===============================================================
# 6. Build Model â€” FULL FINETUNE (No LoRA)
# ===============================================================
def build_model(tokenizer):
    print("[INFO] Load base:", MODEL_NAME)
    model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)

    # KoBART decoder start token
    model.config.decoder_start_token_id = tokenizer.eos_token_id

    # ============ Decoding ì„¤ì • (í›ˆë ¨Â·ê²€ì¦ìš©) ============
    # â†’ ëª¨ë‘ training ì„¹ì…˜ ê¸°ì¤€ìœ¼ë¡œë§Œ ë§ì¶˜ë‹¤
    GEN_MAX       = cfg["training"]["generation_max_length"]           # ì˜ˆ: 40 or 64
    GEN_BEAMS     = cfg["training"]["generation_num_beams"]            # ì˜ˆ: 1
    GEN_NO_REPEAT = cfg["training"]["generation_no_repeat_ngram_size"] # ì˜ˆ: 3
    

    # Trainerê°€ evalì—ì„œ generate() í˜¸ì¶œí•  ë•Œ ì°¸ê³ í•˜ëŠ” ê°’ë“¤
    model.config.max_length = GEN_MAX
    model.config.num_beams = GEN_BEAMS
    model.config.no_repeat_ngram_size = GEN_NO_REPEAT
    

    # KoBARTì— special tokens ë°˜ì˜
    model.resize_token_embeddings(len(tokenizer))

    if torch.cuda.is_available():
        model = model.cuda()

    return model


# ===============================================================
# 7. Optuna Objective
# ===============================================================
def objective(trial, seed, tokenizer, train_tok, dev_tok):
    hp_cfg = cfg["optuna"]["search_space"]

    hp = {
        "learning_rate": trial.suggest_float(
            "learning_rate",
            float(hp_cfg["learning_rate"][0]),
            float(hp_cfg["learning_rate"][1]),
        ),
        "warmup_ratio": trial.suggest_categorical(
            "warmup_ratio",
            hp_cfg["warmup_ratio"],
        ),
        "num_train_epochs": trial.suggest_categorical(
            "num_train_epochs",
            hp_cfg["num_train_epochs"],
        ),
    }

    fix_seed(seed)
    torch.cuda.empty_cache()

    wandb.init(
        project=cfg["wandb"]["project"],
        entity=cfg["wandb"]["entity"],
        name=f"v4_nolora_seed{seed}_trial{trial.number}",
        mode=cfg["wandb"]["mode"],
        reinit=True,
        config={"seed": seed, "trial": trial.number, **hp},
        group=f"seed_{seed}",
    )

    model = build_model(tokenizer)

    # === train/eval generation ì„¤ì • (í›ˆë ¨ ì•ˆì •ìš© ê¸°ë³¸ê°’) ===
    GEN_MAX       = cfg["training"]["generation_max_length"]       # ë³´í†µ 40~64
    GEN_BEAMS     = cfg["training"]["generation_num_beams"]        # ë³´í†µ 1 ë˜ëŠ” 4
    GEN_NO_REPEAT = cfg["training"]["generation_no_repeat_ngram_size"]


    out_dir = os.path.join(OUTPUT_DIR, f"seed_{seed}", f"trial_{trial.number}")
    os.makedirs(out_dir, exist_ok=True)

    args = Seq2SeqTrainingArguments(
        output_dir=out_dir,
        seed=seed,

        per_device_train_batch_size=cfg["training"]["per_device_train_batch_size"],
        gradient_accumulation_steps=cfg["training"]["gradient_accumulation_steps"],

        num_train_epochs=hp["num_train_epochs"],
        learning_rate=hp["learning_rate"],
        warmup_ratio=hp["warmup_ratio"],

        weight_decay=cfg["training"]["weight_decay"],
        lr_scheduler_type=cfg["training"]["lr_scheduler_type"],
        optim=cfg["training"]["optim"],

        evaluation_strategy="epoch",
        save_strategy=cfg["training"]["save_strategy"],
        save_total_limit=cfg["training"]["save_total_limit"],
        load_best_model_at_end=True,

        fp16=cfg["training"]["fp16"],
        bf16=cfg["training"]["bf16"],
        predict_with_generate=True,

               # ğŸ”¥ train/evalì—ì„œ ì“¸ ë””ì½”ë”© ì„¸íŒ… (config.training ê¸°ì¤€)
        generation_max_length=cfg["training"]["generation_max_length"],              # ex) 40
        


        metric_for_best_model="rougeL",
        greater_is_better=True,

        logging_steps=cfg["training"]["logging_steps"],
        report_to=["wandb"],
        eval_accumulation_steps=1,
        remove_unused_columns=False,
    )


    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True,
        label_pad_token_id=-100,
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=args,
        train_dataset=train_tok,
        eval_dataset=dev_tok,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=build_rouge_fn(tokenizer),
    )

    trainer.train()

    eval_metrics = trainer.evaluate()
    score = float(eval_metrics["eval_rougeL"])

    trial.set_user_attr("score", score)

    wandb.log({"eval/rougeL": score})
    wandb.finish()

    del trainer, model
    gc.collect()
    torch.cuda.empty_cache()

    return score

# ===============================================================
# 8. Main
# ===============================================================
def run_seeds():
    tokenizer, ENC_MAX, DEC_MAX = build_tokenizer()
    train_dataset, dev_dataset = load_datasets()

    # fname / topic ì œê±°ëŠ” Dataset ë ˆë²¨ì—ì„œ í•œ ë²ˆ ë” ì•ˆì „í•˜ê²Œ
    for col in ["fname", "topic", "__index_level_0__"]:
        if col in train_dataset.column_names:
            train_dataset = train_dataset.remove_columns(col)
        if col in dev_dataset.column_names:
            dev_dataset = dev_dataset.remove_columns(col)

    preprocess = preprocess_builder(tokenizer, ENC_MAX, DEC_MAX)
    train_tok = train_dataset.map(preprocess, batched=True)
    dev_tok   = dev_dataset.map(preprocess, batched=True)

    # ğŸ”¥ ì—¬ê¸°ì„œ ì›ì¸ ì œê±°: ë” ì´ìƒ í•„ìš” ì—†ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì œê±°
    drop_cols = []
    for c in ["dialogue", "summary"]:
        if c in train_tok.column_names:
            drop_cols.append(c)
    if drop_cols:
        print(f"[INFO] Removing unused text columns from tokenized datasets: {drop_cols}")
        train_tok = train_tok.remove_columns(drop_cols)
        dev_tok   = dev_tok.remove_columns(drop_cols)

    for seed in SEED_LIST:
        print(f"\n======================= SEED {seed} =======================")
        fix_seed(seed)

        seed_dir = os.path.join(OUTPUT_DIR, f"seed_{seed}")
        os.makedirs(seed_dir, exist_ok=True)

        study = optuna.create_study(direction=cfg["optuna"]["direction"])
        study.optimize(
            lambda t: objective(t, seed, tokenizer, train_tok, dev_tok),
            n_trials=cfg["optuna"]["n_trials"],
        )

        df = pd.DataFrame(
            [
                {
                    "seed": seed,
                    "trial": t.number,
                    "score": t.user_attrs.get("score"),
                    **t.params,
                }
                for t in study.trials
            ]
        )
        out_csv = os.path.join(seed_dir, "trial_scores.csv")
        df.to_csv(out_csv, index=False)
        print(f"[INFO] Saved trial scores â†’ {out_csv}")
        print(f"[INFO] Best Score (SEED {seed}): {study.best_value:.4f}")


if __name__ == "__main__":
    run_seeds()
